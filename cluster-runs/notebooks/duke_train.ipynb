{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def add_iscrowd(path):\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for i, annot in enumerate(data['annotations']):\n",
    "        annot['iscrowd'] = 0\n",
    "        data['annotations'][i] = annot\n",
    "\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "base = ...\n",
    "\n",
    "for ds in ['duke_train', 'duke_val', 'manual_maxar_val', 'fake_maxar_train', 'fake_maxar_val']:\n",
    "    path = os.path.join(base, ds, 'labels.json') \n",
    "    add_iscrowd(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# Common Libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register and check datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.getcwd(), '..', 'datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "for d, ds in product([\"train\", \"val\"], ['duke']):\n",
    "    ds_path = os.path.join(os.getcwd(), f'{ds}_{d}', 'data')\n",
    "    json_path = os.path.join(os.getcwd(), f'{ds}_{d}', 'labels.json')\n",
    "    ds_name = f'{ds}_{d}'\n",
    "\n",
    "    if ds == 'overfit':\n",
    "        continue\n",
    "\n",
    "    if ds_name in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(ds_name)\n",
    "        MetadataCatalog.remove(ds_name)\n",
    "\n",
    "    register_coco_instances(ds_name, {}, json_path, ds_path)\n",
    "\n",
    "\n",
    "ds_name = 'iscrowd_manual_maxar_val'   \n",
    "ds_path = os.path.join(os.getcwd(), f'{ds_name}', 'data')\n",
    "json_path = os.path.join(os.getcwd(), f'{ds_name}', 'labels.json')\n",
    "\n",
    "if ds_name in DatasetCatalog.list():\n",
    "    DatasetCatalog.remove(ds_name)\n",
    "    MetadataCatalog.remove(ds_name)\n",
    "\n",
    "register_coco_instances(ds_name, {}, json_path, ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = 'duke_train'\n",
    "val_dataset = 'duke_val'\n",
    "train_metadata = MetadataCatalog.get(train_dataset)\n",
    "val_metadata = MetadataCatalog.get(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dataset_dicts = DatasetCatalog.get(train_dataset)\n",
    "tower_metadata = MetadataCatalog.get(train_dataset)\n",
    "\n",
    "num_examples = 5\n",
    "for d in random.sample(dataset_dicts, num_examples):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=tower_metadata, scale=0.8, instance_mode=1)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    # cv2.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.imshow(out.get_image()[:, :, ::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.evaluation import inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "\n",
    "class EvalTrainer(DefaultTrainer):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        self.same_data_loader = build_detection_test_loader(DatasetCatalog.get(cfg.DATASETS.TEST1),\n",
    "                                                            mapper=DatasetMapper(cfg, is_train=False))\n",
    "        self.same_data_eval = COCOEvaluator(cfg.DATASETS.TEST1)\n",
    "        self.manual_maxar_loader = build_detection_test_loader(DatasetCatalog.get(cfg.DATASETS.TEST2),\n",
    "                                                            mapper=DatasetMapper(cfg, is_train=False))\n",
    "        self.manual_maxar_eval = COCOEvaluator(cfg.DATASETS.TEST2)\n",
    "\n",
    "\n",
    "    def after_step(self):\n",
    "        super().after_step()\n",
    "\n",
    "        if self.iter % self.cfg.TEST.INTERVAL == 0:\n",
    "            \n",
    "            same_data_results = inference_on_dataset(self.model, \n",
    "                                                     self.same_data_loader,\n",
    "                                                     self.same_data_eval)\n",
    "\n",
    "\n",
    "            with open('results_samedata_'+str(self.cfg.DATASETS.TRAIN)+str(self.iter)+'.json', 'w') as out:\n",
    "                json.dump(same_data_results, out)                                   \n",
    "\n",
    "            results = inference_on_dataset(self.model, \n",
    "                                                     self.manual_maxar_loader,\n",
    "                                                     self.manual_maxar_eval)\n",
    "\n",
    "            with open('results_manualsdata_'+str(self.cfg.DATASETS.TRAIN)+str(self.iter)+'.json', 'w') as out:\n",
    "                json.dump(results, out)                                   \n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "def do_train(train_dataset='duke'):\n",
    "\n",
    "    cfg = get_cfg() # Model Confi\n",
    "\n",
    "\n",
    "    frcnn= 'faster_rcnn_R_101_FPN_3x.yaml'\n",
    "\n",
    "    # From Detectron2 Model Zoo\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/\"+frcnn))    # https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-Detection/retinanet_R_101_FPN_3x.yaml\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/\"+frcnn)  # Pre-trained Model Weights\n",
    "\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2  # Number of CPUs to load the data into Detectron2 - 2 for Colab\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2    # Detectron2 default 16 with 8 GPUs, so 16/8 = 2 for 1 GPU\n",
    "\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2   # for R-CNN Models\n",
    "\n",
    "    cfg.defrost()\n",
    "\n",
    "    cfg.DATASETS.TRAIN = (train_dataset+'_train')\n",
    "    cfg.DATASETS.TEST1 = (train_dataset+'_val')\n",
    "    cfg.DATASETS.TEST2 = ('manual_maxar_val')\n",
    "    cfg.TEST.INTERVAL = 30\n",
    "    model_name = str(date.today()) + '_dummy'\n",
    "    cfg.OUTPUT_DIR = '/content/drive/MyDrive/PyPSA_Africa_images/models/' + model_name\n",
    "\n",
    "    cfg.freeze()\n",
    "\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    trainer = EvalTrainer(cfg) \n",
    "    trainer.resume_or_load(resume=False)\n",
    "\n",
    "    trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9bef397ef5e7b2d367b3209d88d895d60e5cf9367d4dd0bd56847f579153646"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
